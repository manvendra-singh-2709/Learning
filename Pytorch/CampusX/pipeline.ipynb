{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c743b7fc",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dbb49206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available.\n",
      "Using MPS: 1\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available.\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.mps.is_available():\n",
    "    print(\"MPS available.\")\n",
    "    print(f\"Using MPS: {torch.mps.device_count()}\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"Using CPU.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "torch.set_default_device(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7ebce492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f42a5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['id', 'Unnamed: 32'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af3bf8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "05964fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "49bb3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "15c8307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train).to(dtype=torch.float32, device=device)\n",
    "x_test_tensor = torch.from_numpy(x_test).to(dtype=torch.float32, device=device)\n",
    "y_train_tensor = torch.from_numpy(y_train).to(dtype=torch.float32, device=device)\n",
    "y_test_tensor = torch.from_numpy(y_test).to(dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f3e7d",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1bc483c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "\n",
    "    def __init__(self, x: torch.Tensor):\n",
    "        self.weights = torch.rand(x.shape[1], 1, dtype = torch.float32, requires_grad = True, device=device)\n",
    "        self.bias = torch.zeros(1, 1, dtype = torch.float32, requires_grad = True, device=device)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = torch.matmul(x, self.weights) + self.bias\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        return y_pred\n",
    "    \n",
    "    def loss_function(self, y_pred: torch.Tensor, y: torch.Tensor):\n",
    "        #Clamp predictions to avoid log(0)\n",
    "        epsilon = 1e-7\n",
    "        y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = -(y* torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred)).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf99e937",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "93cd2969",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df6210",
   "metadata": {},
   "source": [
    "# Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "00bb2e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 3.207179307937622\n",
      "Epoch: 2, Loss: 3.0473341941833496\n",
      "Epoch: 3, Loss: 2.8788464069366455\n",
      "Epoch: 4, Loss: 2.7082901000976562\n",
      "Epoch: 5, Loss: 2.5326600074768066\n",
      "Epoch: 6, Loss: 2.351226329803467\n",
      "Epoch: 7, Loss: 2.1679840087890625\n",
      "Epoch: 8, Loss: 1.9861539602279663\n",
      "Epoch: 9, Loss: 1.8063030242919922\n",
      "Epoch: 10, Loss: 1.6349130868911743\n",
      "Epoch: 11, Loss: 1.468701958656311\n",
      "Epoch: 12, Loss: 1.3158557415008545\n",
      "Epoch: 13, Loss: 1.1793692111968994\n",
      "Epoch: 14, Loss: 1.0622143745422363\n",
      "Epoch: 15, Loss: 0.9666118025779724\n",
      "Epoch: 16, Loss: 0.8932411670684814\n",
      "Epoch: 17, Loss: 0.8404352068901062\n",
      "Epoch: 18, Loss: 0.8043656349182129\n",
      "Epoch: 19, Loss: 0.7803551554679871\n",
      "Epoch: 20, Loss: 0.7642470598220825\n",
      "Epoch: 21, Loss: 0.7530145645141602\n",
      "Epoch: 22, Loss: 0.744718611240387\n",
      "Epoch: 23, Loss: 0.7382098436355591\n",
      "Epoch: 24, Loss: 0.7328335046768188\n",
      "Epoch: 25, Loss: 0.72822105884552\n",
      "Epoch: 26, Loss: 0.7241616249084473\n",
      "Epoch: 27, Loss: 0.720528781414032\n",
      "Epoch: 28, Loss: 0.7172425389289856\n",
      "Epoch: 29, Loss: 0.7142484188079834\n",
      "Epoch: 30, Loss: 0.7115063071250916\n",
      "Epoch: 31, Loss: 0.7089851498603821\n",
      "Epoch: 32, Loss: 0.7066598534584045\n",
      "Epoch: 33, Loss: 0.7045095562934875\n",
      "Epoch: 34, Loss: 0.7025160193443298\n",
      "Epoch: 35, Loss: 0.7006640434265137\n",
      "Epoch: 36, Loss: 0.6989399194717407\n",
      "Epoch: 37, Loss: 0.6973318457603455\n",
      "Epoch: 38, Loss: 0.6958296298980713\n",
      "Epoch: 39, Loss: 0.6944236159324646\n",
      "Epoch: 40, Loss: 0.6931057572364807\n",
      "Epoch: 41, Loss: 0.6918686032295227\n",
      "Epoch: 42, Loss: 0.6907055377960205\n",
      "Epoch: 43, Loss: 0.6896107792854309\n",
      "Epoch: 44, Loss: 0.6885788440704346\n",
      "Epoch: 45, Loss: 0.6876048445701599\n",
      "Epoch: 46, Loss: 0.6866846084594727\n",
      "Epoch: 47, Loss: 0.6858139038085938\n",
      "Epoch: 48, Loss: 0.6849895119667053\n",
      "Epoch: 49, Loss: 0.6842077970504761\n",
      "Epoch: 50, Loss: 0.6834660768508911\n",
      "Epoch: 51, Loss: 0.6827613115310669\n",
      "Epoch: 52, Loss: 0.6820912957191467\n",
      "Epoch: 53, Loss: 0.68145352602005\n",
      "Epoch: 54, Loss: 0.6808462142944336\n",
      "Epoch: 55, Loss: 0.6802670359611511\n",
      "Epoch: 56, Loss: 0.679714560508728\n",
      "Epoch: 57, Loss: 0.6791868209838867\n",
      "Epoch: 58, Loss: 0.6786826848983765\n",
      "Epoch: 59, Loss: 0.6782005429267883\n",
      "Epoch: 60, Loss: 0.6777390241622925\n",
      "Epoch: 61, Loss: 0.6772971153259277\n",
      "Epoch: 62, Loss: 0.6768737435340881\n",
      "Epoch: 63, Loss: 0.6764676570892334\n",
      "Epoch: 64, Loss: 0.6760781407356262\n",
      "Epoch: 65, Loss: 0.6757041811943054\n",
      "Epoch: 66, Loss: 0.6753449440002441\n",
      "Epoch: 67, Loss: 0.6749997138977051\n",
      "Epoch: 68, Loss: 0.6746677160263062\n",
      "Epoch: 69, Loss: 0.6743482947349548\n",
      "Epoch: 70, Loss: 0.6740407943725586\n",
      "Epoch: 71, Loss: 0.6737446188926697\n",
      "Epoch: 72, Loss: 0.6734592914581299\n",
      "Epoch: 73, Loss: 0.6731842756271362\n",
      "Epoch: 74, Loss: 0.672918975353241\n",
      "Epoch: 75, Loss: 0.6726629734039307\n",
      "Epoch: 76, Loss: 0.6724159121513367\n",
      "Epoch: 77, Loss: 0.672177255153656\n",
      "Epoch: 78, Loss: 0.6719468235969543\n",
      "Epoch: 79, Loss: 0.6717239618301392\n",
      "Epoch: 80, Loss: 0.6715087294578552\n",
      "Epoch: 81, Loss: 0.6713004112243652\n",
      "Epoch: 82, Loss: 0.6710988283157349\n",
      "Epoch: 83, Loss: 0.6709038019180298\n",
      "Epoch: 84, Loss: 0.6707149147987366\n",
      "Epoch: 85, Loss: 0.6705319285392761\n",
      "Epoch: 86, Loss: 0.6703546047210693\n",
      "Epoch: 87, Loss: 0.6701828241348267\n",
      "Epoch: 88, Loss: 0.6700162887573242\n",
      "Epoch: 89, Loss: 0.6698546409606934\n",
      "Epoch: 90, Loss: 0.6696979403495789\n",
      "Epoch: 91, Loss: 0.6695457696914673\n",
      "Epoch: 92, Loss: 0.6693981885910034\n",
      "Epoch: 93, Loss: 0.669254720211029\n",
      "Epoch: 94, Loss: 0.669115424156189\n",
      "Epoch: 95, Loss: 0.668980062007904\n",
      "Epoch: 96, Loss: 0.6688485145568848\n",
      "Epoch: 97, Loss: 0.668720543384552\n",
      "Epoch: 98, Loss: 0.6685962677001953\n",
      "Epoch: 99, Loss: 0.6684752702713013\n",
      "Epoch: 100, Loss: 0.6683575510978699\n",
      "Epoch: 101, Loss: 0.6682429909706116\n",
      "Epoch: 102, Loss: 0.668131411075592\n",
      "Epoch: 103, Loss: 0.668022871017456\n",
      "Epoch: 104, Loss: 0.6679170727729797\n",
      "Epoch: 105, Loss: 0.6678139567375183\n",
      "Epoch: 106, Loss: 0.6677135825157166\n",
      "Epoch: 107, Loss: 0.6676157712936401\n",
      "Epoch: 108, Loss: 0.6675202250480652\n",
      "Epoch: 109, Loss: 0.6674273014068604\n",
      "Epoch: 110, Loss: 0.6673365235328674\n",
      "Epoch: 111, Loss: 0.6672478914260864\n",
      "Epoch: 112, Loss: 0.6671615839004517\n",
      "Epoch: 113, Loss: 0.6670772433280945\n",
      "Epoch: 114, Loss: 0.6669949293136597\n",
      "Epoch: 115, Loss: 0.6669145226478577\n",
      "Epoch: 116, Loss: 0.666836142539978\n",
      "Epoch: 117, Loss: 0.6667593717575073\n",
      "Epoch: 118, Loss: 0.6666844487190247\n",
      "Epoch: 119, Loss: 0.6666111946105957\n",
      "Epoch: 120, Loss: 0.6665396690368652\n",
      "Epoch: 121, Loss: 0.6664696931838989\n",
      "Epoch: 122, Loss: 0.6664013862609863\n",
      "Epoch: 123, Loss: 0.6663344502449036\n",
      "Epoch: 124, Loss: 0.6662690043449402\n",
      "Epoch: 125, Loss: 0.6662049889564514\n",
      "Epoch: 126, Loss: 0.666142463684082\n",
      "Epoch: 127, Loss: 0.6660810708999634\n",
      "Epoch: 128, Loss: 0.6660211682319641\n",
      "Epoch: 129, Loss: 0.6659624576568604\n",
      "Epoch: 130, Loss: 0.6659049391746521\n",
      "Epoch: 131, Loss: 0.6658486127853394\n",
      "Epoch: 132, Loss: 0.6657935380935669\n",
      "Epoch: 133, Loss: 0.6657394766807556\n",
      "Epoch: 134, Loss: 0.6656864881515503\n",
      "Epoch: 135, Loss: 0.6656346321105957\n",
      "Epoch: 136, Loss: 0.6655838489532471\n",
      "Epoch: 137, Loss: 0.6655340790748596\n",
      "Epoch: 138, Loss: 0.665485143661499\n",
      "Epoch: 139, Loss: 0.6654372811317444\n",
      "Epoch: 140, Loss: 0.6653903722763062\n",
      "Epoch: 141, Loss: 0.6653442978858948\n",
      "Epoch: 142, Loss: 0.6652991771697998\n",
      "Epoch: 143, Loss: 0.6652547717094421\n",
      "Epoch: 144, Loss: 0.6652113199234009\n",
      "Epoch: 145, Loss: 0.6651687026023865\n",
      "Epoch: 146, Loss: 0.6651268005371094\n",
      "Epoch: 147, Loss: 0.6650857329368591\n",
      "Epoch: 148, Loss: 0.665045440196991\n",
      "Epoch: 149, Loss: 0.6650058031082153\n",
      "Epoch: 150, Loss: 0.6649669408798218\n",
      "Epoch: 151, Loss: 0.6649287343025208\n",
      "Epoch: 152, Loss: 0.664891242980957\n",
      "Epoch: 153, Loss: 0.6648544073104858\n",
      "Epoch: 154, Loss: 0.664818286895752\n",
      "Epoch: 155, Loss: 0.6647826433181763\n",
      "Epoch: 156, Loss: 0.6647477746009827\n",
      "Epoch: 157, Loss: 0.664713442325592\n",
      "Epoch: 158, Loss: 0.6646797060966492\n",
      "Epoch: 159, Loss: 0.664646565914154\n",
      "Epoch: 160, Loss: 0.6646140217781067\n",
      "Epoch: 161, Loss: 0.6645819544792175\n",
      "Epoch: 162, Loss: 0.6645504832267761\n",
      "Epoch: 163, Loss: 0.6645195484161377\n",
      "Epoch: 164, Loss: 0.6644890904426575\n",
      "Epoch: 165, Loss: 0.6644591689109802\n",
      "Epoch: 166, Loss: 0.6644297242164612\n",
      "Epoch: 167, Loss: 0.6644007563591003\n",
      "Epoch: 168, Loss: 0.6643722653388977\n",
      "Epoch: 169, Loss: 0.6643441915512085\n",
      "Epoch: 170, Loss: 0.6643166542053223\n",
      "Epoch: 171, Loss: 0.6642894744873047\n",
      "Epoch: 172, Loss: 0.6642627716064453\n",
      "Epoch: 173, Loss: 0.6642364859580994\n",
      "Epoch: 174, Loss: 0.6642106175422668\n",
      "Epoch: 175, Loss: 0.6641851663589478\n",
      "Epoch: 176, Loss: 0.6641600728034973\n",
      "Epoch: 177, Loss: 0.6641353964805603\n",
      "Epoch: 178, Loss: 0.6641110777854919\n",
      "Epoch: 179, Loss: 0.6640872359275818\n",
      "Epoch: 180, Loss: 0.6640636920928955\n",
      "Epoch: 181, Loss: 0.6640404462814331\n",
      "Epoch: 182, Loss: 0.6640175580978394\n",
      "Epoch: 183, Loss: 0.6639951467514038\n",
      "Epoch: 184, Loss: 0.6639728546142578\n",
      "Epoch: 185, Loss: 0.66395103931427\n",
      "Epoch: 186, Loss: 0.6639295816421509\n",
      "Epoch: 187, Loss: 0.6639083623886108\n",
      "Epoch: 188, Loss: 0.6638875007629395\n",
      "Epoch: 189, Loss: 0.6638668179512024\n",
      "Epoch: 190, Loss: 0.6638466119766235\n",
      "Epoch: 191, Loss: 0.6638265252113342\n",
      "Epoch: 192, Loss: 0.6638068556785583\n",
      "Epoch: 193, Loss: 0.6637874245643616\n",
      "Epoch: 194, Loss: 0.6637682914733887\n",
      "Epoch: 195, Loss: 0.6637493968009949\n",
      "Epoch: 196, Loss: 0.663730800151825\n",
      "Epoch: 197, Loss: 0.6637125015258789\n",
      "Epoch: 198, Loss: 0.6636943221092224\n",
      "Epoch: 199, Loss: 0.6636764407157898\n",
      "Epoch: 200, Loss: 0.6636587977409363\n",
      "Epoch: 201, Loss: 0.6636414527893066\n",
      "Epoch: 202, Loss: 0.6636243462562561\n",
      "Epoch: 203, Loss: 0.6636074781417847\n",
      "Epoch: 204, Loss: 0.6635909080505371\n",
      "Epoch: 205, Loss: 0.6635743975639343\n",
      "Epoch: 206, Loss: 0.6635580658912659\n",
      "Epoch: 207, Loss: 0.6635421514511108\n",
      "Epoch: 208, Loss: 0.6635263562202454\n",
      "Epoch: 209, Loss: 0.6635107398033142\n",
      "Epoch: 210, Loss: 0.6634954214096069\n",
      "Epoch: 211, Loss: 0.663480281829834\n",
      "Epoch: 212, Loss: 0.6634652614593506\n",
      "Epoch: 213, Loss: 0.6634505391120911\n",
      "Epoch: 214, Loss: 0.6634358763694763\n",
      "Epoch: 215, Loss: 0.6634214520454407\n",
      "Epoch: 216, Loss: 0.6634072065353394\n",
      "Epoch: 217, Loss: 0.6633932590484619\n",
      "Epoch: 218, Loss: 0.663379430770874\n",
      "Epoch: 219, Loss: 0.6633656620979309\n",
      "Epoch: 220, Loss: 0.6633521914482117\n",
      "Epoch: 221, Loss: 0.663338840007782\n",
      "Epoch: 222, Loss: 0.6633256673812866\n",
      "Epoch: 223, Loss: 0.6633127331733704\n",
      "Epoch: 224, Loss: 0.6632997989654541\n",
      "Epoch: 225, Loss: 0.6632871031761169\n",
      "Epoch: 226, Loss: 0.6632745862007141\n",
      "Epoch: 227, Loss: 0.6632623076438904\n",
      "Epoch: 228, Loss: 0.6632499694824219\n",
      "Epoch: 229, Loss: 0.6632379293441772\n",
      "Epoch: 230, Loss: 0.6632258892059326\n",
      "Epoch: 231, Loss: 0.6632141470909119\n",
      "Epoch: 232, Loss: 0.6632024645805359\n",
      "Epoch: 233, Loss: 0.6631909608840942\n",
      "Epoch: 234, Loss: 0.6631795763969421\n",
      "Epoch: 235, Loss: 0.6631683111190796\n",
      "Epoch: 236, Loss: 0.6631571650505066\n",
      "Epoch: 237, Loss: 0.6631461977958679\n",
      "Epoch: 238, Loss: 0.6631353497505188\n",
      "Epoch: 239, Loss: 0.6631246209144592\n",
      "Epoch: 240, Loss: 0.6631139516830444\n",
      "Epoch: 241, Loss: 0.663103461265564\n",
      "Epoch: 242, Loss: 0.663093090057373\n",
      "Epoch: 243, Loss: 0.6630828976631165\n",
      "Epoch: 244, Loss: 0.6630727052688599\n",
      "Epoch: 245, Loss: 0.6630626320838928\n",
      "Epoch: 246, Loss: 0.6630528569221497\n",
      "Epoch: 247, Loss: 0.6630429625511169\n",
      "Epoch: 248, Loss: 0.6630333065986633\n",
      "Epoch: 249, Loss: 0.6630237102508545\n",
      "Epoch: 250, Loss: 0.6630142331123352\n",
      "Epoch: 251, Loss: 0.6630048751831055\n",
      "Epoch: 252, Loss: 0.6629955768585205\n",
      "Epoch: 253, Loss: 0.6629863381385803\n",
      "Epoch: 254, Loss: 0.6629772782325745\n",
      "Epoch: 255, Loss: 0.6629683375358582\n",
      "Epoch: 256, Loss: 0.6629593968391418\n",
      "Epoch: 257, Loss: 0.6629506349563599\n",
      "Epoch: 258, Loss: 0.6629419922828674\n",
      "Epoch: 259, Loss: 0.662933349609375\n",
      "Epoch: 260, Loss: 0.6629247665405273\n",
      "Epoch: 261, Loss: 0.662916362285614\n",
      "Epoch: 262, Loss: 0.6629080176353455\n",
      "Epoch: 263, Loss: 0.6628998517990112\n",
      "Epoch: 264, Loss: 0.662891685962677\n",
      "Epoch: 265, Loss: 0.6628835797309875\n",
      "Epoch: 266, Loss: 0.6628755331039429\n",
      "Epoch: 267, Loss: 0.6628676056861877\n",
      "Epoch: 268, Loss: 0.6628597974777222\n",
      "Epoch: 269, Loss: 0.6628521084785461\n",
      "Epoch: 270, Loss: 0.6628443002700806\n",
      "Epoch: 271, Loss: 0.6628367900848389\n",
      "Epoch: 272, Loss: 0.6628292202949524\n",
      "Epoch: 273, Loss: 0.6628217697143555\n",
      "Epoch: 274, Loss: 0.6628144383430481\n",
      "Epoch: 275, Loss: 0.662807047367096\n",
      "Epoch: 276, Loss: 0.6627998352050781\n",
      "Epoch: 277, Loss: 0.6627926826477051\n",
      "Epoch: 278, Loss: 0.6627855896949768\n",
      "Epoch: 279, Loss: 0.6627784967422485\n",
      "Epoch: 280, Loss: 0.6627715826034546\n",
      "Epoch: 281, Loss: 0.6627647876739502\n",
      "Epoch: 282, Loss: 0.6627578139305115\n",
      "Epoch: 283, Loss: 0.6627510786056519\n",
      "Epoch: 284, Loss: 0.662744402885437\n",
      "Epoch: 285, Loss: 0.6627378463745117\n",
      "Epoch: 286, Loss: 0.6627312302589417\n",
      "Epoch: 287, Loss: 0.6627246737480164\n",
      "Epoch: 288, Loss: 0.6627181768417358\n",
      "Epoch: 289, Loss: 0.6627118587493896\n",
      "Epoch: 290, Loss: 0.6627055406570435\n",
      "Epoch: 291, Loss: 0.6626992225646973\n",
      "Epoch: 292, Loss: 0.6626930832862854\n",
      "Epoch: 293, Loss: 0.6626868844032288\n",
      "Epoch: 294, Loss: 0.6626807451248169\n",
      "Epoch: 295, Loss: 0.6626746654510498\n",
      "Epoch: 296, Loss: 0.6626687049865723\n",
      "Epoch: 297, Loss: 0.6626627445220947\n",
      "Epoch: 298, Loss: 0.662656843662262\n",
      "Epoch: 299, Loss: 0.662651002407074\n",
      "Epoch: 300, Loss: 0.6626452207565308\n",
      "Epoch: 301, Loss: 0.6626394987106323\n",
      "Epoch: 302, Loss: 0.6626337170600891\n",
      "Epoch: 303, Loss: 0.662628173828125\n",
      "Epoch: 304, Loss: 0.6626225709915161\n",
      "Epoch: 305, Loss: 0.6626169681549072\n",
      "Epoch: 306, Loss: 0.6626115441322327\n",
      "Epoch: 307, Loss: 0.6626061201095581\n",
      "Epoch: 308, Loss: 0.6626006960868835\n",
      "Epoch: 309, Loss: 0.662595272064209\n",
      "Epoch: 310, Loss: 0.662589967250824\n",
      "Epoch: 311, Loss: 0.6625847816467285\n",
      "Epoch: 312, Loss: 0.6625794768333435\n",
      "Epoch: 313, Loss: 0.6625743508338928\n",
      "Epoch: 314, Loss: 0.6625691056251526\n",
      "Epoch: 315, Loss: 0.6625641584396362\n",
      "Epoch: 316, Loss: 0.6625590324401855\n",
      "Epoch: 317, Loss: 0.6625540256500244\n",
      "Epoch: 318, Loss: 0.6625490784645081\n",
      "Epoch: 319, Loss: 0.6625441312789917\n",
      "Epoch: 320, Loss: 0.6625392436981201\n",
      "Epoch: 321, Loss: 0.6625344157218933\n",
      "Epoch: 322, Loss: 0.6625295877456665\n",
      "Epoch: 323, Loss: 0.6625248193740845\n",
      "Epoch: 324, Loss: 0.6625200510025024\n",
      "Epoch: 325, Loss: 0.6625153422355652\n",
      "Epoch: 326, Loss: 0.6625106930732727\n",
      "Epoch: 327, Loss: 0.662506103515625\n",
      "Epoch: 328, Loss: 0.6625015139579773\n",
      "Epoch: 329, Loss: 0.6624969840049744\n",
      "Epoch: 330, Loss: 0.6624923944473267\n",
      "Epoch: 331, Loss: 0.6624879837036133\n",
      "Epoch: 332, Loss: 0.6624835729598999\n",
      "Epoch: 333, Loss: 0.6624791026115417\n",
      "Epoch: 334, Loss: 0.6624746322631836\n",
      "Epoch: 335, Loss: 0.6624703407287598\n",
      "Epoch: 336, Loss: 0.6624660491943359\n",
      "Epoch: 337, Loss: 0.6624617576599121\n",
      "Epoch: 338, Loss: 0.6624574065208435\n",
      "Epoch: 339, Loss: 0.6624532341957092\n",
      "Epoch: 340, Loss: 0.662449061870575\n",
      "Epoch: 341, Loss: 0.6624448895454407\n",
      "Epoch: 342, Loss: 0.662440836429596\n",
      "Epoch: 343, Loss: 0.6624366641044617\n",
      "Epoch: 344, Loss: 0.6624325513839722\n",
      "Epoch: 345, Loss: 0.6624284982681274\n",
      "Epoch: 346, Loss: 0.6624245047569275\n",
      "Epoch: 347, Loss: 0.6624205112457275\n",
      "Epoch: 348, Loss: 0.6624165177345276\n",
      "Epoch: 349, Loss: 0.662412703037262\n",
      "Epoch: 350, Loss: 0.6624086499214172\n",
      "Epoch: 351, Loss: 0.6624048948287964\n",
      "Epoch: 352, Loss: 0.6624009609222412\n",
      "Epoch: 353, Loss: 0.6623972058296204\n",
      "Epoch: 354, Loss: 0.6623934507369995\n",
      "Epoch: 355, Loss: 0.6623896360397339\n",
      "Epoch: 356, Loss: 0.662385880947113\n",
      "Epoch: 357, Loss: 0.662382185459137\n",
      "Epoch: 358, Loss: 0.6623784899711609\n",
      "Epoch: 359, Loss: 0.6623747944831848\n",
      "Epoch: 360, Loss: 0.6623712182044983\n",
      "Epoch: 361, Loss: 0.6623675227165222\n",
      "Epoch: 362, Loss: 0.6623640060424805\n",
      "Epoch: 363, Loss: 0.6623603701591492\n",
      "Epoch: 364, Loss: 0.6623568534851074\n",
      "Epoch: 365, Loss: 0.6623533368110657\n",
      "Epoch: 366, Loss: 0.6623498201370239\n",
      "Epoch: 367, Loss: 0.662346363067627\n",
      "Epoch: 368, Loss: 0.6623428463935852\n",
      "Epoch: 369, Loss: 0.662339448928833\n",
      "Epoch: 370, Loss: 0.662335991859436\n",
      "Epoch: 371, Loss: 0.6623325943946838\n",
      "Epoch: 372, Loss: 0.6623292565345764\n",
      "Epoch: 373, Loss: 0.6623259782791138\n",
      "Epoch: 374, Loss: 0.6623226404190063\n",
      "Epoch: 375, Loss: 0.6623193025588989\n",
      "Epoch: 376, Loss: 0.6623159646987915\n",
      "Epoch: 377, Loss: 0.6623128056526184\n",
      "Epoch: 378, Loss: 0.6623095273971558\n",
      "Epoch: 379, Loss: 0.6623061895370483\n",
      "Epoch: 380, Loss: 0.66230309009552\n",
      "Epoch: 381, Loss: 0.6622998714447021\n",
      "Epoch: 382, Loss: 0.6622966527938843\n",
      "Epoch: 383, Loss: 0.662293553352356\n",
      "Epoch: 384, Loss: 0.6622903943061829\n",
      "Epoch: 385, Loss: 0.6622872948646545\n",
      "Epoch: 386, Loss: 0.6622841954231262\n",
      "Epoch: 387, Loss: 0.6622812151908875\n",
      "Epoch: 388, Loss: 0.6622781157493591\n",
      "Epoch: 389, Loss: 0.6622750759124756\n",
      "Epoch: 390, Loss: 0.6622720956802368\n",
      "Epoch: 391, Loss: 0.6622690558433533\n",
      "Epoch: 392, Loss: 0.6622661352157593\n",
      "Epoch: 393, Loss: 0.6622631549835205\n",
      "Epoch: 394, Loss: 0.6622602343559265\n",
      "Epoch: 395, Loss: 0.6622572541236877\n",
      "Epoch: 396, Loss: 0.6622543334960938\n",
      "Epoch: 397, Loss: 0.6622514128684998\n",
      "Epoch: 398, Loss: 0.6622484922409058\n",
      "Epoch: 399, Loss: 0.6622456312179565\n",
      "Epoch: 400, Loss: 0.6622428894042969\n",
      "Epoch: 401, Loss: 0.6622400879859924\n",
      "Epoch: 402, Loss: 0.6622372269630432\n",
      "Epoch: 403, Loss: 0.6622343063354492\n",
      "Epoch: 404, Loss: 0.6622316241264343\n",
      "Epoch: 405, Loss: 0.6622288227081299\n",
      "Epoch: 406, Loss: 0.6622260212898254\n",
      "Epoch: 407, Loss: 0.6622233390808105\n",
      "Epoch: 408, Loss: 0.6622205972671509\n",
      "Epoch: 409, Loss: 0.6622178554534912\n",
      "Epoch: 410, Loss: 0.6622151732444763\n",
      "Epoch: 411, Loss: 0.6622124314308167\n",
      "Epoch: 412, Loss: 0.6622098088264465\n",
      "Epoch: 413, Loss: 0.6622071862220764\n",
      "Epoch: 414, Loss: 0.6622046232223511\n",
      "Epoch: 415, Loss: 0.6622018814086914\n",
      "Epoch: 416, Loss: 0.6621993184089661\n",
      "Epoch: 417, Loss: 0.662196695804596\n",
      "Epoch: 418, Loss: 0.6621941328048706\n",
      "Epoch: 419, Loss: 0.6621915102005005\n",
      "Epoch: 420, Loss: 0.6621889472007751\n",
      "Epoch: 421, Loss: 0.6621863842010498\n",
      "Epoch: 422, Loss: 0.662183940410614\n",
      "Epoch: 423, Loss: 0.6621813774108887\n",
      "Epoch: 424, Loss: 0.6621788740158081\n",
      "Epoch: 425, Loss: 0.6621763706207275\n",
      "Epoch: 426, Loss: 0.662173867225647\n",
      "Epoch: 427, Loss: 0.6621713638305664\n",
      "Epoch: 428, Loss: 0.6621689200401306\n",
      "Epoch: 429, Loss: 0.6621664762496948\n",
      "Epoch: 430, Loss: 0.6621639728546143\n",
      "Epoch: 431, Loss: 0.662161648273468\n",
      "Epoch: 432, Loss: 0.662159264087677\n",
      "Epoch: 433, Loss: 0.6621568202972412\n",
      "Epoch: 434, Loss: 0.662154495716095\n",
      "Epoch: 435, Loss: 0.6621520519256592\n",
      "Epoch: 436, Loss: 0.6621496677398682\n",
      "Epoch: 437, Loss: 0.6621472835540771\n",
      "Epoch: 438, Loss: 0.6621449589729309\n",
      "Epoch: 439, Loss: 0.6621426343917847\n",
      "Epoch: 440, Loss: 0.6621403098106384\n",
      "Epoch: 441, Loss: 0.662138044834137\n",
      "Epoch: 442, Loss: 0.662135660648346\n",
      "Epoch: 443, Loss: 0.6621333360671997\n",
      "Epoch: 444, Loss: 0.6621310710906982\n",
      "Epoch: 445, Loss: 0.6621288061141968\n",
      "Epoch: 446, Loss: 0.6621266007423401\n",
      "Epoch: 447, Loss: 0.6621243953704834\n",
      "Epoch: 448, Loss: 0.6621221899986267\n",
      "Epoch: 449, Loss: 0.6621198654174805\n",
      "Epoch: 450, Loss: 0.6621176600456238\n",
      "Epoch: 451, Loss: 0.6621153950691223\n",
      "Epoch: 452, Loss: 0.6621132493019104\n",
      "Epoch: 453, Loss: 0.6621109843254089\n",
      "Epoch: 454, Loss: 0.6621088981628418\n",
      "Epoch: 455, Loss: 0.6621066331863403\n",
      "Epoch: 456, Loss: 0.6621044874191284\n",
      "Epoch: 457, Loss: 0.6621024012565613\n",
      "Epoch: 458, Loss: 0.6621003150939941\n",
      "Epoch: 459, Loss: 0.6620981097221375\n",
      "Epoch: 460, Loss: 0.6620959639549255\n",
      "Epoch: 461, Loss: 0.6620938181877136\n",
      "Epoch: 462, Loss: 0.6620916724205017\n",
      "Epoch: 463, Loss: 0.6620896458625793\n",
      "Epoch: 464, Loss: 0.6620875597000122\n",
      "Epoch: 465, Loss: 0.6620854735374451\n",
      "Epoch: 466, Loss: 0.6620833873748779\n",
      "Epoch: 467, Loss: 0.662081241607666\n",
      "Epoch: 468, Loss: 0.6620793342590332\n",
      "Epoch: 469, Loss: 0.6620771884918213\n",
      "Epoch: 470, Loss: 0.6620752215385437\n",
      "Epoch: 471, Loss: 0.6620731353759766\n",
      "Epoch: 472, Loss: 0.662071168422699\n",
      "Epoch: 473, Loss: 0.6620692014694214\n",
      "Epoch: 474, Loss: 0.6620670557022095\n",
      "Epoch: 475, Loss: 0.6620651483535767\n",
      "Epoch: 476, Loss: 0.6620631814002991\n",
      "Epoch: 477, Loss: 0.6620610952377319\n",
      "Epoch: 478, Loss: 0.6620591282844543\n",
      "Epoch: 479, Loss: 0.6620572805404663\n",
      "Epoch: 480, Loss: 0.662055253982544\n",
      "Epoch: 481, Loss: 0.6620533466339111\n",
      "Epoch: 482, Loss: 0.6620513796806335\n",
      "Epoch: 483, Loss: 0.662049412727356\n",
      "Epoch: 484, Loss: 0.6620474457740784\n",
      "Epoch: 485, Loss: 0.6620455980300903\n",
      "Epoch: 486, Loss: 0.6620436906814575\n",
      "Epoch: 487, Loss: 0.6620417237281799\n",
      "Epoch: 488, Loss: 0.6620398163795471\n",
      "Epoch: 489, Loss: 0.6620379686355591\n",
      "Epoch: 490, Loss: 0.6620360612869263\n",
      "Epoch: 491, Loss: 0.6620341539382935\n",
      "Epoch: 492, Loss: 0.6620323657989502\n",
      "Epoch: 493, Loss: 0.6620303988456726\n",
      "Epoch: 494, Loss: 0.6620286107063293\n",
      "Epoch: 495, Loss: 0.6620267629623413\n",
      "Epoch: 496, Loss: 0.6620249152183533\n",
      "Epoch: 497, Loss: 0.6620230674743652\n",
      "Epoch: 498, Loss: 0.6620212197303772\n",
      "Epoch: 499, Loss: 0.6620193719863892\n",
      "Epoch: 500, Loss: 0.6620175242424011\n",
      "Epoch: 501, Loss: 0.6620157361030579\n",
      "Epoch: 502, Loss: 0.6620139479637146\n",
      "Epoch: 503, Loss: 0.6620121598243713\n",
      "Epoch: 504, Loss: 0.6620103120803833\n",
      "Epoch: 505, Loss: 0.6620085835456848\n",
      "Epoch: 506, Loss: 0.6620067954063416\n",
      "Epoch: 507, Loss: 0.6620050668716431\n",
      "Epoch: 508, Loss: 0.662003219127655\n",
      "Epoch: 509, Loss: 0.6620014905929565\n",
      "Epoch: 510, Loss: 0.6619997024536133\n",
      "Epoch: 511, Loss: 0.6619979739189148\n",
      "Epoch: 512, Loss: 0.6619961857795715\n",
      "Epoch: 513, Loss: 0.6619945764541626\n",
      "Epoch: 514, Loss: 0.6619927287101746\n",
      "Epoch: 515, Loss: 0.6619910597801208\n",
      "Epoch: 516, Loss: 0.6619892716407776\n",
      "Epoch: 517, Loss: 0.6619875431060791\n",
      "Epoch: 518, Loss: 0.6619858741760254\n",
      "Epoch: 519, Loss: 0.6619842052459717\n",
      "Epoch: 520, Loss: 0.6619824171066284\n",
      "Epoch: 521, Loss: 0.6619807481765747\n",
      "Epoch: 522, Loss: 0.661979079246521\n",
      "Epoch: 523, Loss: 0.6619774103164673\n",
      "Epoch: 524, Loss: 0.6619756817817688\n",
      "Epoch: 525, Loss: 0.6619740128517151\n",
      "Epoch: 526, Loss: 0.6619723439216614\n",
      "Epoch: 527, Loss: 0.6619707942008972\n",
      "Epoch: 528, Loss: 0.6619690656661987\n",
      "Epoch: 529, Loss: 0.6619674563407898\n",
      "Epoch: 530, Loss: 0.6619657278060913\n",
      "Epoch: 531, Loss: 0.6619641184806824\n",
      "Epoch: 532, Loss: 0.6619624495506287\n",
      "Epoch: 533, Loss: 0.6619608402252197\n",
      "Epoch: 534, Loss: 0.6619592308998108\n",
      "Epoch: 535, Loss: 0.6619576215744019\n",
      "Epoch: 536, Loss: 0.6619560122489929\n",
      "Epoch: 537, Loss: 0.661954402923584\n",
      "Epoch: 538, Loss: 0.661952793598175\n",
      "Epoch: 539, Loss: 0.6619511246681213\n",
      "Epoch: 540, Loss: 0.6619495749473572\n",
      "Epoch: 541, Loss: 0.6619479656219482\n",
      "Epoch: 542, Loss: 0.6619464159011841\n",
      "Epoch: 543, Loss: 0.6619448065757751\n",
      "Epoch: 544, Loss: 0.661943256855011\n",
      "Epoch: 545, Loss: 0.6619417071342468\n",
      "Epoch: 546, Loss: 0.6619401574134827\n",
      "Epoch: 547, Loss: 0.661938488483429\n",
      "Epoch: 548, Loss: 0.6619369387626648\n",
      "Epoch: 549, Loss: 0.6619354486465454\n",
      "Epoch: 550, Loss: 0.6619338989257812\n",
      "Epoch: 551, Loss: 0.6619322896003723\n",
      "Epoch: 552, Loss: 0.6619307994842529\n",
      "Epoch: 553, Loss: 0.6619292497634888\n",
      "Epoch: 554, Loss: 0.6619277000427246\n",
      "Epoch: 555, Loss: 0.6619261503219604\n",
      "Epoch: 556, Loss: 0.6619247198104858\n",
      "Epoch: 557, Loss: 0.6619231700897217\n",
      "Epoch: 558, Loss: 0.6619216203689575\n",
      "Epoch: 559, Loss: 0.6619201302528381\n",
      "Epoch: 560, Loss: 0.6619186997413635\n",
      "Epoch: 561, Loss: 0.6619170904159546\n",
      "Epoch: 562, Loss: 0.6619156002998352\n",
      "Epoch: 563, Loss: 0.6619141101837158\n",
      "Epoch: 564, Loss: 0.6619126796722412\n",
      "Epoch: 565, Loss: 0.661911129951477\n",
      "Epoch: 566, Loss: 0.6619096398353577\n",
      "Epoch: 567, Loss: 0.6619081497192383\n",
      "Epoch: 568, Loss: 0.6619067192077637\n",
      "Epoch: 569, Loss: 0.6619052886962891\n",
      "Epoch: 570, Loss: 0.6619037389755249\n",
      "Epoch: 571, Loss: 0.6619022488594055\n",
      "Epoch: 572, Loss: 0.6619008779525757\n",
      "Epoch: 573, Loss: 0.6618993878364563\n",
      "Epoch: 574, Loss: 0.6618980169296265\n",
      "Epoch: 575, Loss: 0.6618965268135071\n",
      "Epoch: 576, Loss: 0.6618950963020325\n",
      "Epoch: 577, Loss: 0.6618936657905579\n",
      "Epoch: 578, Loss: 0.6618921160697937\n",
      "Epoch: 579, Loss: 0.6618907451629639\n",
      "Epoch: 580, Loss: 0.6618893146514893\n",
      "Epoch: 581, Loss: 0.6618878841400146\n",
      "Epoch: 582, Loss: 0.66188645362854\n",
      "Epoch: 583, Loss: 0.6618850231170654\n",
      "Epoch: 584, Loss: 0.6618836522102356\n",
      "Epoch: 585, Loss: 0.6618821620941162\n",
      "Epoch: 586, Loss: 0.6618807911872864\n",
      "Epoch: 587, Loss: 0.6618794798851013\n",
      "Epoch: 588, Loss: 0.6618779897689819\n",
      "Epoch: 589, Loss: 0.6618766784667969\n",
      "Epoch: 590, Loss: 0.6618752479553223\n",
      "Epoch: 591, Loss: 0.6618738770484924\n",
      "Epoch: 592, Loss: 0.6618725061416626\n",
      "Epoch: 593, Loss: 0.6618710160255432\n",
      "Epoch: 594, Loss: 0.6618697047233582\n",
      "Epoch: 595, Loss: 0.6618683338165283\n",
      "Epoch: 596, Loss: 0.6618669033050537\n",
      "Epoch: 597, Loss: 0.6618655920028687\n",
      "Epoch: 598, Loss: 0.6618642210960388\n",
      "Epoch: 599, Loss: 0.661862850189209\n",
      "Epoch: 600, Loss: 0.6618614792823792\n",
      "Epoch: 601, Loss: 0.6618601083755493\n",
      "Epoch: 602, Loss: 0.6618587970733643\n",
      "Epoch: 603, Loss: 0.6618574857711792\n",
      "Epoch: 604, Loss: 0.6618560552597046\n",
      "Epoch: 605, Loss: 0.6618548631668091\n",
      "Epoch: 606, Loss: 0.6618534326553345\n",
      "Epoch: 607, Loss: 0.6618521213531494\n",
      "Epoch: 608, Loss: 0.6618507504463196\n",
      "Epoch: 609, Loss: 0.6618494391441345\n",
      "Epoch: 610, Loss: 0.6618480682373047\n",
      "Epoch: 611, Loss: 0.6618467569351196\n",
      "Epoch: 612, Loss: 0.6618453860282898\n",
      "Epoch: 613, Loss: 0.6618441343307495\n",
      "Epoch: 614, Loss: 0.6618427634239197\n",
      "Epoch: 615, Loss: 0.6618415713310242\n",
      "Epoch: 616, Loss: 0.6618402004241943\n",
      "Epoch: 617, Loss: 0.661838948726654\n",
      "Epoch: 618, Loss: 0.6618375778198242\n",
      "Epoch: 619, Loss: 0.6618362665176392\n",
      "Epoch: 620, Loss: 0.6618350148200989\n",
      "Epoch: 621, Loss: 0.6618337035179138\n",
      "Epoch: 622, Loss: 0.661832332611084\n",
      "Epoch: 623, Loss: 0.6618311405181885\n",
      "Epoch: 624, Loss: 0.6618297696113586\n",
      "Epoch: 625, Loss: 0.6618285775184631\n",
      "Epoch: 626, Loss: 0.6618272066116333\n",
      "Epoch: 627, Loss: 0.6618260145187378\n",
      "Epoch: 628, Loss: 0.661824643611908\n",
      "Epoch: 629, Loss: 0.6618234515190125\n",
      "Epoch: 630, Loss: 0.6618221402168274\n",
      "Epoch: 631, Loss: 0.6618208885192871\n",
      "Epoch: 632, Loss: 0.661819577217102\n",
      "Epoch: 633, Loss: 0.6618184447288513\n",
      "Epoch: 634, Loss: 0.6618171334266663\n",
      "Epoch: 635, Loss: 0.661815881729126\n",
      "Epoch: 636, Loss: 0.6618145704269409\n",
      "Epoch: 637, Loss: 0.6618134379386902\n",
      "Epoch: 638, Loss: 0.6618121266365051\n",
      "Epoch: 639, Loss: 0.6618109345436096\n",
      "Epoch: 640, Loss: 0.6618096232414246\n",
      "Epoch: 641, Loss: 0.661808431148529\n",
      "Epoch: 642, Loss: 0.661807119846344\n",
      "Epoch: 643, Loss: 0.6618059277534485\n",
      "Epoch: 644, Loss: 0.6618046760559082\n",
      "Epoch: 645, Loss: 0.6618034243583679\n",
      "Epoch: 646, Loss: 0.6618022322654724\n",
      "Epoch: 647, Loss: 0.6618010997772217\n",
      "Epoch: 648, Loss: 0.6617998480796814\n",
      "Epoch: 649, Loss: 0.6617985367774963\n",
      "Epoch: 650, Loss: 0.6617973446846008\n",
      "Epoch: 651, Loss: 0.6617961525917053\n",
      "Epoch: 652, Loss: 0.661794900894165\n",
      "Epoch: 653, Loss: 0.6617937088012695\n",
      "Epoch: 654, Loss: 0.661792516708374\n",
      "Epoch: 655, Loss: 0.6617913246154785\n",
      "Epoch: 656, Loss: 0.6617900729179382\n",
      "Epoch: 657, Loss: 0.6617889404296875\n",
      "Epoch: 658, Loss: 0.661787748336792\n",
      "Epoch: 659, Loss: 0.6617865562438965\n",
      "Epoch: 660, Loss: 0.6617854237556458\n",
      "Epoch: 661, Loss: 0.6617841124534607\n",
      "Epoch: 662, Loss: 0.66178297996521\n",
      "Epoch: 663, Loss: 0.6617817878723145\n",
      "Epoch: 664, Loss: 0.6617806553840637\n",
      "Epoch: 665, Loss: 0.6617794632911682\n",
      "Epoch: 666, Loss: 0.6617782711982727\n",
      "Epoch: 667, Loss: 0.661777138710022\n",
      "Epoch: 668, Loss: 0.6617758870124817\n",
      "Epoch: 669, Loss: 0.661774754524231\n",
      "Epoch: 670, Loss: 0.6617735028266907\n",
      "Epoch: 671, Loss: 0.6617724299430847\n",
      "Epoch: 672, Loss: 0.6617712378501892\n",
      "Epoch: 673, Loss: 0.6617701053619385\n",
      "Epoch: 674, Loss: 0.661768913269043\n",
      "Epoch: 675, Loss: 0.6617677807807922\n",
      "Epoch: 676, Loss: 0.6617666482925415\n",
      "Epoch: 677, Loss: 0.6617655158042908\n",
      "Epoch: 678, Loss: 0.6617642641067505\n",
      "Epoch: 679, Loss: 0.6617631316184998\n",
      "Epoch: 680, Loss: 0.661761999130249\n",
      "Epoch: 681, Loss: 0.6617608666419983\n",
      "Epoch: 682, Loss: 0.6617597341537476\n",
      "Epoch: 683, Loss: 0.661758542060852\n",
      "Epoch: 684, Loss: 0.6617574691772461\n",
      "Epoch: 685, Loss: 0.6617562770843506\n",
      "Epoch: 686, Loss: 0.6617552042007446\n",
      "Epoch: 687, Loss: 0.6617540121078491\n",
      "Epoch: 688, Loss: 0.6617528796195984\n",
      "Epoch: 689, Loss: 0.661751925945282\n",
      "Epoch: 690, Loss: 0.6617506742477417\n",
      "Epoch: 691, Loss: 0.661749541759491\n",
      "Epoch: 692, Loss: 0.6617484092712402\n",
      "Epoch: 693, Loss: 0.6617472767829895\n",
      "Epoch: 694, Loss: 0.6617461442947388\n",
      "Epoch: 695, Loss: 0.6617451310157776\n",
      "Epoch: 696, Loss: 0.6617439985275269\n",
      "Epoch: 697, Loss: 0.6617428660392761\n",
      "Epoch: 698, Loss: 0.6617417931556702\n",
      "Epoch: 699, Loss: 0.6617406606674194\n",
      "Epoch: 700, Loss: 0.6617395281791687\n",
      "Epoch: 701, Loss: 0.6617384552955627\n",
      "Epoch: 702, Loss: 0.661737322807312\n",
      "Epoch: 703, Loss: 0.6617361903190613\n",
      "Epoch: 704, Loss: 0.6617351174354553\n",
      "Epoch: 705, Loss: 0.6617341041564941\n",
      "Epoch: 706, Loss: 0.6617329716682434\n",
      "Epoch: 707, Loss: 0.6617318987846375\n",
      "Epoch: 708, Loss: 0.6617307662963867\n",
      "Epoch: 709, Loss: 0.6617296934127808\n",
      "Epoch: 710, Loss: 0.6617286801338196\n",
      "Epoch: 711, Loss: 0.6617275476455688\n",
      "Epoch: 712, Loss: 0.6617264747619629\n",
      "Epoch: 713, Loss: 0.6617253422737122\n",
      "Epoch: 714, Loss: 0.6617242693901062\n",
      "Epoch: 715, Loss: 0.6617231965065002\n",
      "Epoch: 716, Loss: 0.6617221832275391\n",
      "Epoch: 717, Loss: 0.6617211103439331\n",
      "Epoch: 718, Loss: 0.6617199778556824\n",
      "Epoch: 719, Loss: 0.6617189049720764\n",
      "Epoch: 720, Loss: 0.6617177724838257\n",
      "Epoch: 721, Loss: 0.6617168188095093\n",
      "Epoch: 722, Loss: 0.6617158055305481\n",
      "Epoch: 723, Loss: 0.6617146134376526\n",
      "Epoch: 724, Loss: 0.6617136597633362\n",
      "Epoch: 725, Loss: 0.6617125868797302\n",
      "Epoch: 726, Loss: 0.6617115139961243\n",
      "Epoch: 727, Loss: 0.6617104411125183\n",
      "Epoch: 728, Loss: 0.6617094278335571\n",
      "Epoch: 729, Loss: 0.661708414554596\n",
      "Epoch: 730, Loss: 0.6617074012756348\n",
      "Epoch: 731, Loss: 0.6617063283920288\n",
      "Epoch: 732, Loss: 0.6617052555084229\n",
      "Epoch: 733, Loss: 0.6617041826248169\n",
      "Epoch: 734, Loss: 0.6617031693458557\n",
      "Epoch: 735, Loss: 0.6617020964622498\n",
      "Epoch: 736, Loss: 0.6617010235786438\n",
      "Epoch: 737, Loss: 0.6617000699043274\n",
      "Epoch: 738, Loss: 0.6616990566253662\n",
      "Epoch: 739, Loss: 0.6616979837417603\n",
      "Epoch: 740, Loss: 0.6616969704627991\n",
      "Epoch: 741, Loss: 0.6616958975791931\n",
      "Epoch: 742, Loss: 0.6616949439048767\n",
      "Epoch: 743, Loss: 0.6616938710212708\n",
      "Epoch: 744, Loss: 0.6616928577423096\n",
      "Epoch: 745, Loss: 0.6616918444633484\n",
      "Epoch: 746, Loss: 0.6616907715797424\n",
      "Epoch: 747, Loss: 0.6616898775100708\n",
      "Epoch: 748, Loss: 0.6616887450218201\n",
      "Epoch: 749, Loss: 0.6616877317428589\n",
      "Epoch: 750, Loss: 0.6616867780685425\n",
      "Epoch: 751, Loss: 0.6616857647895813\n",
      "Epoch: 752, Loss: 0.6616847515106201\n",
      "Epoch: 753, Loss: 0.6616837978363037\n",
      "Epoch: 754, Loss: 0.6616827845573425\n",
      "Epoch: 755, Loss: 0.6616817116737366\n",
      "Epoch: 756, Loss: 0.6616807579994202\n",
      "Epoch: 757, Loss: 0.661679744720459\n",
      "Epoch: 758, Loss: 0.661678671836853\n",
      "Epoch: 759, Loss: 0.6616777777671814\n",
      "Epoch: 760, Loss: 0.6616767048835754\n",
      "Epoch: 761, Loss: 0.661675751209259\n",
      "Epoch: 762, Loss: 0.6616747379302979\n",
      "Epoch: 763, Loss: 0.6616737246513367\n",
      "Epoch: 764, Loss: 0.661672830581665\n",
      "Epoch: 765, Loss: 0.6616717576980591\n",
      "Epoch: 766, Loss: 0.6616707444190979\n",
      "Epoch: 767, Loss: 0.6616697907447815\n",
      "Epoch: 768, Loss: 0.6616687774658203\n",
      "Epoch: 769, Loss: 0.6616678237915039\n",
      "Epoch: 770, Loss: 0.6616669297218323\n",
      "Epoch: 771, Loss: 0.6616659164428711\n",
      "Epoch: 772, Loss: 0.6616649031639099\n",
      "Epoch: 773, Loss: 0.6616639494895935\n",
      "Epoch: 774, Loss: 0.6616629958152771\n",
      "Epoch: 775, Loss: 0.6616619229316711\n",
      "Epoch: 776, Loss: 0.6616610288619995\n",
      "Epoch: 777, Loss: 0.6616600751876831\n",
      "Epoch: 778, Loss: 0.6616590619087219\n",
      "Epoch: 779, Loss: 0.6616581678390503\n",
      "Epoch: 780, Loss: 0.6616571545600891\n",
      "Epoch: 781, Loss: 0.6616562008857727\n",
      "Epoch: 782, Loss: 0.6616553068161011\n",
      "Epoch: 783, Loss: 0.6616542935371399\n",
      "Epoch: 784, Loss: 0.6616533398628235\n",
      "Epoch: 785, Loss: 0.6616524457931519\n",
      "Epoch: 786, Loss: 0.6616513729095459\n",
      "Epoch: 787, Loss: 0.6616504788398743\n",
      "Epoch: 788, Loss: 0.6616494655609131\n",
      "Epoch: 789, Loss: 0.6616486310958862\n",
      "Epoch: 790, Loss: 0.661647617816925\n",
      "Epoch: 791, Loss: 0.6616466641426086\n",
      "Epoch: 792, Loss: 0.6616457104682922\n",
      "Epoch: 793, Loss: 0.6616448163986206\n",
      "Epoch: 794, Loss: 0.6616438031196594\n",
      "Epoch: 795, Loss: 0.6616429090499878\n",
      "Epoch: 796, Loss: 0.6616419553756714\n",
      "Epoch: 797, Loss: 0.6616411209106445\n",
      "Epoch: 798, Loss: 0.6616400480270386\n",
      "Epoch: 799, Loss: 0.6616391539573669\n",
      "Epoch: 800, Loss: 0.6616382598876953\n",
      "Epoch: 801, Loss: 0.6616372466087341\n",
      "Epoch: 802, Loss: 0.6616364121437073\n",
      "Epoch: 803, Loss: 0.6616353988647461\n",
      "Epoch: 804, Loss: 0.6616344451904297\n",
      "Epoch: 805, Loss: 0.6616335511207581\n",
      "Epoch: 806, Loss: 0.6616326570510864\n",
      "Epoch: 807, Loss: 0.6616317629814148\n",
      "Epoch: 808, Loss: 0.6616308689117432\n",
      "Epoch: 809, Loss: 0.6616299152374268\n",
      "Epoch: 810, Loss: 0.6616290211677551\n",
      "Epoch: 811, Loss: 0.6616279482841492\n",
      "Epoch: 812, Loss: 0.6616270542144775\n",
      "Epoch: 813, Loss: 0.6616262197494507\n",
      "Epoch: 814, Loss: 0.6616252660751343\n",
      "Epoch: 815, Loss: 0.6616243720054626\n",
      "Epoch: 816, Loss: 0.6616234183311462\n",
      "Epoch: 817, Loss: 0.6616225242614746\n",
      "Epoch: 818, Loss: 0.6616216897964478\n",
      "Epoch: 819, Loss: 0.6616207361221313\n",
      "Epoch: 820, Loss: 0.6616198420524597\n",
      "Epoch: 821, Loss: 0.6616188883781433\n",
      "Epoch: 822, Loss: 0.6616179943084717\n",
      "Epoch: 823, Loss: 0.6616171002388\n",
      "Epoch: 824, Loss: 0.6616162061691284\n",
      "Epoch: 825, Loss: 0.6616153120994568\n",
      "Epoch: 826, Loss: 0.6616144180297852\n",
      "Epoch: 827, Loss: 0.6616135835647583\n",
      "Epoch: 828, Loss: 0.6616127490997314\n",
      "Epoch: 829, Loss: 0.661611795425415\n",
      "Epoch: 830, Loss: 0.6616108417510986\n",
      "Epoch: 831, Loss: 0.6616100072860718\n",
      "Epoch: 832, Loss: 0.6616091132164001\n",
      "Epoch: 833, Loss: 0.6616082191467285\n",
      "Epoch: 834, Loss: 0.6616072654724121\n",
      "Epoch: 835, Loss: 0.6616063714027405\n",
      "Epoch: 836, Loss: 0.6616055369377136\n",
      "Epoch: 837, Loss: 0.661604642868042\n",
      "Epoch: 838, Loss: 0.6616038084030151\n",
      "Epoch: 839, Loss: 0.6616029143333435\n",
      "Epoch: 840, Loss: 0.6616020202636719\n",
      "Epoch: 841, Loss: 0.6616011261940002\n",
      "Epoch: 842, Loss: 0.6616001725196838\n",
      "Epoch: 843, Loss: 0.6615993976593018\n",
      "Epoch: 844, Loss: 0.6615985631942749\n",
      "Epoch: 845, Loss: 0.6615976095199585\n",
      "Epoch: 846, Loss: 0.6615967154502869\n",
      "Epoch: 847, Loss: 0.66159588098526\n",
      "Epoch: 848, Loss: 0.6615950465202332\n",
      "Epoch: 849, Loss: 0.6615941524505615\n",
      "Epoch: 850, Loss: 0.6615932583808899\n",
      "Epoch: 851, Loss: 0.661592423915863\n",
      "Epoch: 852, Loss: 0.6615915894508362\n",
      "Epoch: 853, Loss: 0.6615906953811646\n",
      "Epoch: 854, Loss: 0.6615898609161377\n",
      "Epoch: 855, Loss: 0.6615889668464661\n",
      "Epoch: 856, Loss: 0.6615881323814392\n",
      "Epoch: 857, Loss: 0.6615872979164124\n",
      "Epoch: 858, Loss: 0.6615864038467407\n",
      "Epoch: 859, Loss: 0.6615855693817139\n",
      "Epoch: 860, Loss: 0.6615846753120422\n",
      "Epoch: 861, Loss: 0.6615838408470154\n",
      "Epoch: 862, Loss: 0.6615830063819885\n",
      "Epoch: 863, Loss: 0.6615821719169617\n",
      "Epoch: 864, Loss: 0.6615813374519348\n",
      "Epoch: 865, Loss: 0.6615804433822632\n",
      "Epoch: 866, Loss: 0.6615796685218811\n",
      "Epoch: 867, Loss: 0.6615787744522095\n",
      "Epoch: 868, Loss: 0.6615779399871826\n",
      "Epoch: 869, Loss: 0.6615771055221558\n",
      "Epoch: 870, Loss: 0.6615762710571289\n",
      "Epoch: 871, Loss: 0.661575436592102\n",
      "Epoch: 872, Loss: 0.6615745425224304\n",
      "Epoch: 873, Loss: 0.6615737676620483\n",
      "Epoch: 874, Loss: 0.6615729331970215\n",
      "Epoch: 875, Loss: 0.6615720391273499\n",
      "Epoch: 876, Loss: 0.661571204662323\n",
      "Epoch: 877, Loss: 0.6615703701972961\n",
      "Epoch: 878, Loss: 0.6615695357322693\n",
      "Epoch: 879, Loss: 0.6615687012672424\n",
      "Epoch: 880, Loss: 0.6615678668022156\n",
      "Epoch: 881, Loss: 0.6615670919418335\n",
      "Epoch: 882, Loss: 0.6615662574768066\n",
      "Epoch: 883, Loss: 0.6615654230117798\n",
      "Epoch: 884, Loss: 0.6615645885467529\n",
      "Epoch: 885, Loss: 0.6615636944770813\n",
      "Epoch: 886, Loss: 0.6615629196166992\n",
      "Epoch: 887, Loss: 0.6615620851516724\n",
      "Epoch: 888, Loss: 0.6615612506866455\n",
      "Epoch: 889, Loss: 0.6615604758262634\n",
      "Epoch: 890, Loss: 0.6615596413612366\n",
      "Epoch: 891, Loss: 0.6615588665008545\n",
      "Epoch: 892, Loss: 0.6615580320358276\n",
      "Epoch: 893, Loss: 0.6615572571754456\n",
      "Epoch: 894, Loss: 0.6615564227104187\n",
      "Epoch: 895, Loss: 0.6615555882453918\n",
      "Epoch: 896, Loss: 0.661554753780365\n",
      "Epoch: 897, Loss: 0.6615540385246277\n",
      "Epoch: 898, Loss: 0.6615532040596008\n",
      "Epoch: 899, Loss: 0.661552369594574\n",
      "Epoch: 900, Loss: 0.6615515947341919\n",
      "Epoch: 901, Loss: 0.661550760269165\n",
      "Epoch: 902, Loss: 0.6615499258041382\n",
      "Epoch: 903, Loss: 0.6615491509437561\n",
      "Epoch: 904, Loss: 0.661548376083374\n",
      "Epoch: 905, Loss: 0.6615476012229919\n",
      "Epoch: 906, Loss: 0.6615467667579651\n",
      "Epoch: 907, Loss: 0.661545991897583\n",
      "Epoch: 908, Loss: 0.6615450978279114\n",
      "Epoch: 909, Loss: 0.6615443229675293\n",
      "Epoch: 910, Loss: 0.661543607711792\n",
      "Epoch: 911, Loss: 0.6615427732467651\n",
      "Epoch: 912, Loss: 0.6615419983863831\n",
      "Epoch: 913, Loss: 0.6615411639213562\n",
      "Epoch: 914, Loss: 0.6615404486656189\n",
      "Epoch: 915, Loss: 0.6615396738052368\n",
      "Epoch: 916, Loss: 0.6615387797355652\n",
      "Epoch: 917, Loss: 0.6615380048751831\n",
      "Epoch: 918, Loss: 0.661537230014801\n",
      "Epoch: 919, Loss: 0.6615365147590637\n",
      "Epoch: 920, Loss: 0.6615356802940369\n",
      "Epoch: 921, Loss: 0.66153484582901\n",
      "Epoch: 922, Loss: 0.6615341305732727\n",
      "Epoch: 923, Loss: 0.6615333557128906\n",
      "Epoch: 924, Loss: 0.6615325808525085\n",
      "Epoch: 925, Loss: 0.6615318059921265\n",
      "Epoch: 926, Loss: 0.6615309715270996\n",
      "Epoch: 927, Loss: 0.6615302562713623\n",
      "Epoch: 928, Loss: 0.6615294218063354\n",
      "Epoch: 929, Loss: 0.6615286469459534\n",
      "Epoch: 930, Loss: 0.6615278720855713\n",
      "Epoch: 931, Loss: 0.661527156829834\n",
      "Epoch: 932, Loss: 0.6615263819694519\n",
      "Epoch: 933, Loss: 0.6615254878997803\n",
      "Epoch: 934, Loss: 0.661524772644043\n",
      "Epoch: 935, Loss: 0.6615241169929504\n",
      "Epoch: 936, Loss: 0.6615232825279236\n",
      "Epoch: 937, Loss: 0.6615225076675415\n",
      "Epoch: 938, Loss: 0.6615217328071594\n",
      "Epoch: 939, Loss: 0.6615209579467773\n",
      "Epoch: 940, Loss: 0.66152024269104\n",
      "Epoch: 941, Loss: 0.6615195274353027\n",
      "Epoch: 942, Loss: 0.6615186929702759\n",
      "Epoch: 943, Loss: 0.6615179181098938\n",
      "Epoch: 944, Loss: 0.6615172028541565\n",
      "Epoch: 945, Loss: 0.6615164279937744\n",
      "Epoch: 946, Loss: 0.6615156531333923\n",
      "Epoch: 947, Loss: 0.661514937877655\n",
      "Epoch: 948, Loss: 0.661514163017273\n",
      "Epoch: 949, Loss: 0.6615133881568909\n",
      "Epoch: 950, Loss: 0.6615126729011536\n",
      "Epoch: 951, Loss: 0.6615118980407715\n",
      "Epoch: 952, Loss: 0.6615111231803894\n",
      "Epoch: 953, Loss: 0.6615104675292969\n",
      "Epoch: 954, Loss: 0.6615096926689148\n",
      "Epoch: 955, Loss: 0.6615089774131775\n",
      "Epoch: 956, Loss: 0.6615081429481506\n",
      "Epoch: 957, Loss: 0.6615074276924133\n",
      "Epoch: 958, Loss: 0.661506712436676\n",
      "Epoch: 959, Loss: 0.6615059971809387\n",
      "Epoch: 960, Loss: 0.6615052819252014\n",
      "Epoch: 961, Loss: 0.6615045070648193\n",
      "Epoch: 962, Loss: 0.6615037322044373\n",
      "Epoch: 963, Loss: 0.6615030169487\n",
      "Epoch: 964, Loss: 0.6615022420883179\n",
      "Epoch: 965, Loss: 0.6615015268325806\n",
      "Epoch: 966, Loss: 0.6615008115768433\n",
      "Epoch: 967, Loss: 0.6615000367164612\n",
      "Epoch: 968, Loss: 0.6614993810653687\n",
      "Epoch: 969, Loss: 0.6614986062049866\n",
      "Epoch: 970, Loss: 0.6614978313446045\n",
      "Epoch: 971, Loss: 0.6614971160888672\n",
      "Epoch: 972, Loss: 0.6614964008331299\n",
      "Epoch: 973, Loss: 0.661495566368103\n",
      "Epoch: 974, Loss: 0.6614949107170105\n",
      "Epoch: 975, Loss: 0.661494255065918\n",
      "Epoch: 976, Loss: 0.6614935398101807\n",
      "Epoch: 977, Loss: 0.6614928245544434\n",
      "Epoch: 978, Loss: 0.661492109298706\n",
      "Epoch: 979, Loss: 0.6614913940429688\n",
      "Epoch: 980, Loss: 0.6614906191825867\n",
      "Epoch: 981, Loss: 0.6614899039268494\n",
      "Epoch: 982, Loss: 0.6614891290664673\n",
      "Epoch: 983, Loss: 0.6614884734153748\n",
      "Epoch: 984, Loss: 0.6614877581596375\n",
      "Epoch: 985, Loss: 0.6614869832992554\n",
      "Epoch: 986, Loss: 0.6614863276481628\n",
      "Epoch: 987, Loss: 0.6614855527877808\n",
      "Epoch: 988, Loss: 0.6614848971366882\n",
      "Epoch: 989, Loss: 0.6614841222763062\n",
      "Epoch: 990, Loss: 0.6614834666252136\n",
      "Epoch: 991, Loss: 0.6614827513694763\n",
      "Epoch: 992, Loss: 0.661482036113739\n",
      "Epoch: 993, Loss: 0.6614813208580017\n",
      "Epoch: 994, Loss: 0.6614805459976196\n",
      "Epoch: 995, Loss: 0.6614798903465271\n",
      "Epoch: 996, Loss: 0.661479115486145\n",
      "Epoch: 997, Loss: 0.6614785194396973\n",
      "Epoch: 998, Loss: 0.66147780418396\n",
      "Epoch: 999, Loss: 0.6614770293235779\n",
      "Epoch: 1000, Loss: 0.6614764332771301\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(x_train_tensor)\n",
    "\n",
    "# Define loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward Pass\n",
    "    y_pred = model.forward(x_train_tensor)\n",
    "\n",
    "    # Loss Calculation\n",
    "    loss = model.loss_function(y_pred, y_train_tensor)\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Paramers Update\n",
    "    with torch.no_grad():\n",
    "        model.weights -= lr * model.weights.grad\n",
    "        model.bias -= lr * model.bias.grad\n",
    "    \n",
    "    # Zero Gradients\n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383a264",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94a4ea32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6315789222717285\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model.forward(x_test_tensor)\n",
    "    y_pred = (y_pred > 0.7).float()\n",
    "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
    "    print(f\"Accuracy: {accuracy.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
